{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\final v1.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m random\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m torch\u001b[39m.\u001b[39;49mmanual_seed(seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmanual_seed_all(seed)\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 40\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmanual_seed_all(seed)\n\u001b[0;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmps\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    110\u001b[0m         default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    111\u001b[0m         default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 113\u001b[0m _lazy_call(cb, seed_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:183\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_call\u001b[39m(\u001b[39mcallable\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 183\u001b[0m         \u001b[39mcallable\u001b[39;49m()\n\u001b[0;32m    184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m         \u001b[39m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m         \u001b[39m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    187\u001b[0m         \u001b[39m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[39mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m    110\u001b[0m     default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 111\u001b[0m     default_generator\u001b[39m.\u001b[39;49mmanual_seed(seed)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for dataset and dataloader\n",
    "input_size = 10\n",
    "target_size = 5\n",
    "batch_size = 8\n",
    "\n",
    "# hyperparameters for model\n",
    "num_features = 1  # 입력 시퀀스의 특징 수 (예: open, close 가격)\n",
    "\n",
    "d_model = 64\n",
    "n_heads = 2\n",
    "num_encoder_layers = 2\n",
    "d_ff = 256\n",
    "dropout_rate = 0.1\n",
    "lstm_hidden_size = 128\n",
    "num_lstm_layers = 2\n",
    "\n",
    "# hyperparameters for training, testing\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str, input_window_size, target_window_size):\n",
    "        self.directory = f'C:/Github/DL-FinalProject/csvfiles/{item}/'\n",
    "        self.input_window_size = input_window_size\n",
    "        self.target_window_size = target_window_size\n",
    "        self.columns = [1, 4]\n",
    "        self.data = self.load_data(start_date_str, end_date_str)\n",
    "\n",
    "    def load_data(self, start_date_str, end_date_str):\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        all_data = []\n",
    "\n",
    "        for filename in os.listdir(self.directory):\n",
    "            # Extract date from filename\n",
    "            file_date_str = '-'.join(filename.split('-')[2:]).split('.')[0]\n",
    "            file_date = dt.strptime(file_date_str, '%Y-%m-%d').date()\n",
    "\n",
    "            if start_date <= file_date <= end_date:\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                df = pd.read_csv(file_path, usecols=self.columns)\n",
    "                all_data.append(df)\n",
    "\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_window_size - self.target_window_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx + self.input_window_size + self.target_window_size > len(self.data):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        window_data = self.data.iloc[idx:idx + self.input_window_size + self.target_window_size]\n",
    "        open_prices = window_data.iloc[:, 0]  # Assuming 1st column is 'open' prices\n",
    "        close_prices = window_data.iloc[:, 1]  # Assuming 4th column is 'close' prices\n",
    "        percentage_changes = ((close_prices - open_prices) * 100 / open_prices)\n",
    "        input_data = torch.tensor(percentage_changes.values[:self.input_window_size], dtype=torch.float32)\n",
    "        target_data = torch.tensor(percentage_changes.values[self.input_window_size:], dtype=torch.float32)\n",
    "        return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 클래스\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 시계열 트랜스포머 모델 클래스\n",
    "class TimeSeriesTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model, n_heads, num_encoder_layers, d_ff, dropout_rate, lstm_hidden_size, num_lstm_layers, target_size):\n",
    "        super(TimeSeriesTransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=lstm_hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer to transform LSTM output to match Transformer d_model size\n",
    "        self.linear1 = nn.Linear(lstm_hidden_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_rate)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        self.attention_weights = nn.Parameter(torch.randn(input_size, 1))\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(src)\n",
    "        # lstm_out: [batch_size, sequence_length, lstm_hidden_size]\n",
    "\n",
    "        # Transform LSTM output to match Transformer d_model size\n",
    "        src = self.linear1(lstm_out) * math.sqrt(self.d_model)\n",
    "        # src: [batch_size, sequence_length, d_model]\n",
    "\n",
    "        # Positional Encoding and Transformer\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # output: [batch_size, sequence_length, d_model]\n",
    "\n",
    "        # 가중치 평균 계산을 위한 어텐션 적용\n",
    "        attention_weights = torch.softmax(self.attention_weights, dim=0)\n",
    "        output = torch.matmul(output.transpose(1, 2), attention_weights).squeeze(-1)\n",
    "        # output: [batch_size, d_model]\n",
    "\n",
    "        # 선형 레이어를 통과시켜 최종 출력 생성\n",
    "        output = torch.sigmoid(self.out(output))\n",
    "        # output: [batch_size, target_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_data, target_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 데이터 차원 변경 및 디바이스로 이동\n",
    "        input_data = input_data.unsqueeze(-1).to(device)  # [32, 10] -> [32, 10, 1]\n",
    "        target_data = target_data.to(device) # [32, 5]\n",
    "\n",
    "        # 모델 예측\n",
    "        output = model(input_data)\n",
    "\n",
    "        # 손실 계산 및 역전파\n",
    "        loss = criterion(output, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in test_loader:\n",
    "            # 데이터 차원 변경 및 디바이스로 이동\n",
    "            input_data = input_data.unsqueeze(-1).to(device)  # [32, 10] -> [32, 10, 1]\n",
    "            target_data = target_data.to(device) # [32, 5]\n",
    "\n",
    "            # 모델 예측\n",
    "            output = model(input_data)\n",
    "\n",
    "            # 손실 계산\n",
    "            loss = criterion(output, target_data)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2021-12-01', input_window_size=input_size, target_window_size=target_size)\n",
    "test_dataset = PriceDataset('BTCUSDT', '1m', '2022-01-01', '2022-01-30', input_window_size=input_size, target_window_size=target_size)\n",
    "\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # 학습 데이터 로더\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # 테스트 데이터 로더\n",
    "\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = TimeSeriesTransformerModel(num_features, d_model, n_heads, num_encoder_layers, d_ff, dropout_rate, lstm_hidden_size, num_lstm_layers, target_size)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model found. Initializing a new model.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\final v1.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_test_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     test_loss \u001b[39m=\u001b[39m test(model, test_loader, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\final v1.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m model(input_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# 손실 계산 및 역전파\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/final%20v1.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3096\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3098\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 모델 파일 경로\n",
    "model_file_path = \"best_model.pth\"\n",
    "\n",
    "# 모델 파일이 존재하는지 확인하고, 존재할 경우 모델 로드\n",
    "if os.path.isfile(model_file_path):\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    print(\"Pre-trained model loaded.\")\n",
    "else:\n",
    "    print(\"No pre-trained model found. Initializing a new model.\")\n",
    "\n",
    "# 학습 과정\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = test(model, test_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "    # 테스트 손실이 이전 최소값보다 낮을 때만 모델 저장\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), model_file_path)\n",
    "        print(f\"Model saved at Epoch {epoch + 1}\")\n",
    "\n",
    "    # 학습 과정 시각화\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Testing Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
