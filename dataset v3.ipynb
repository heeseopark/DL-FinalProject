{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters and Lstm hyperparameters\n",
    "window_size = 100 # lstm input size\n",
    "\n",
    "input_window_size = 100\n",
    "\n",
    "target_window_size = 10 # lstm output size\n",
    "\n",
    "hidden_size = 1000\n",
    "\n",
    "num_layers = 4\n",
    "\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input sequence: 100 <- 조절 가능하게 해야함. 그리고 input sequence는 가격 값을 input 받음\n",
    "\n",
    "output sequence: 10 <- 이것 또한 조절 가능하게 해야함. 그리고 output sequence는 상승/하락 binary를 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset v2는 전체 csv file을 합친 후에 dataloader에 전달하려고 한 것인데, 메모리 문제로 load하지 못하는 문제가 있었음.\n",
    "\n",
    "그래서 v3에서는 날짜마다 load 하되, 한 날의 마지막 부분에 data가 짤리는 경우에는 다음 날짜가 존재하는지 확인한 뒤에, 뒤의 data를 가져와서 dataloader에 보내는 것으로 결정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가격 자체가 중요한 것이 아니기 때문에 가격 변동값을 input으로 넣을 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/DL-FinalProject/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_length = 0\n",
    "        for filename in self.filenames:\n",
    "            df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "            total_length += len(df)\n",
    "\n",
    "        # Adjust for the fact that the last few entries in the dataset may not form a complete window\n",
    "        return max(0, total_length - input_window_size - target_window_size + 1)\n",
    "    \n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = max(0, idx - input_window_size - target_window_size + 1)\n",
    "        end_idx = min(idx + 1, len(self.filenames))\n",
    "\n",
    "        tensors = []\n",
    "        for file_idx in range(start_idx, end_idx):\n",
    "            filename = self.filenames[file_idx]\n",
    "            df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "            tensor = torch.tensor(df.values, dtype=torch.float)\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        combined_tensor = torch.cat(tensors, dim=0)\n",
    "        return combined_tensor\n",
    "\n",
    "def sliding_window_percentage(batch):\n",
    "    windows_percentage = []\n",
    "    for tensor in batch:\n",
    "        total_length = tensor.shape[0]\n",
    "        for i in range(total_length - input_window_size - target_window_size + 1):\n",
    "            window = tensor[i:i + input_window_size + target_window_size]\n",
    "            pct_change = ((window[-target_window_size:, 1] - window[:input_window_size, 0]) * 100 / window[:input_window_size, 0])\n",
    "            windows_percentage.append(pct_change)\n",
    "\n",
    "    output_percentage = torch.stack(windows_percentage)\n",
    "    return output_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows in train dataset: 1137937\n",
      "Total windows in test dataset: 1137937\n"
     ]
    }
   ],
   "source": [
    "def count_total_windows(dataset):\n",
    "    total_length = 0\n",
    "    for filename in dataset.filenames:\n",
    "        df = pd.read_csv(filename, usecols=dataset.columns, header=None)\n",
    "        total_length += len(df)\n",
    "\n",
    "    # Adjust for the fact that the last few entries in the dataset may not form a complete window\n",
    "    total_windows = max(0, total_length - input_window_size - target_window_size + 1)\n",
    "    return total_windows\n",
    "\n",
    "# Example usage\n",
    "total_train_windows = count_total_windows(train_dataset)\n",
    "total_test_windows = count_total_windows(test_dataset)\n",
    "\n",
    "print(f\"Total windows in train dataset: {total_train_windows}\")\n",
    "print(f\"Total windows in test dataset: {total_test_windows}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
