{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total windows in train dataset: 1137937\n",
      "Total windows in test dataset: 1137937\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Dataset parameters and Lstm hyperparameters\n",
    "window_size = 100 # lstm input size\n",
    "\n",
    "input_window_size = 100\n",
    "\n",
    "target_window_size = 10 # lstm output size\n",
    "\n",
    "hidden_size = 1000\n",
    "\n",
    "num_layers = 4\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/DL-FinalProject/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_length = 0\n",
    "        for filename in self.filenames:\n",
    "            df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "            total_length += len(df)\n",
    "\n",
    "        # Adjust for the fact that the last few entries in the dataset may not form a complete window\n",
    "        return max(0, total_length - input_window_size - target_window_size + 1)\n",
    "    \n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure index is within bounds\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        # Calculate start and end indices for files to read\n",
    "        start_idx = idx\n",
    "        end_idx = idx + input_window_size + target_window_size\n",
    "\n",
    "        # Adjust if the end index goes beyond the dataset\n",
    "        if end_idx > len(self.filenames):\n",
    "            end_idx = len(self.filenames)\n",
    "\n",
    "        tensors = []\n",
    "        for file_idx in range(start_idx, end_idx):\n",
    "            filename = self.filenames[file_idx]\n",
    "            if os.path.exists(filename):\n",
    "                df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "                if not df.empty:\n",
    "                    tensor = torch.tensor(df.values, dtype=torch.float)\n",
    "                    tensors.append(tensor)\n",
    "\n",
    "        # Check if tensors list is empty\n",
    "        if not tensors:\n",
    "            raise ValueError(f\"No data found for index {idx}\")\n",
    "\n",
    "        combined_tensor = torch.cat(tensors, dim=0)\n",
    "\n",
    "        # Here, ensure that combined_tensor has the expected shape\n",
    "        # and adjust as necessary\n",
    "\n",
    "        return combined_tensor\n",
    "\n",
    "\n",
    "def sliding_window_percentage(batch):\n",
    "    windows_percentage = []\n",
    "    for tensor in batch:\n",
    "        total_length = tensor.shape[0]\n",
    "        for i in range(total_length - input_window_size - target_window_size + 1):\n",
    "            window = tensor[i:i + input_window_size + target_window_size]\n",
    "            pct_change = ((window[-target_window_size:, 1] - window[:input_window_size, 0]) * 100 / window[:input_window_size, 0])\n",
    "            windows_percentage.append(pct_change)\n",
    "\n",
    "    output_percentage = torch.stack(windows_percentage)\n",
    "    return output_percentage\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "train_dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def count_total_windows(dataset):\n",
    "    total_length = 0\n",
    "    for filename in dataset.filenames:\n",
    "        df = pd.read_csv(filename, usecols=dataset.columns, header=None)\n",
    "        total_length += len(df)\n",
    "\n",
    "    # Adjust for the fact that the last few entries in the dataset may not form a complete window\n",
    "    total_windows = max(0, total_length - input_window_size - target_window_size + 1)\n",
    "    return total_windows\n",
    "\n",
    "# Example usage\n",
    "total_train_windows = count_total_windows(train_dataset)\n",
    "total_test_windows = count_total_windows(test_dataset)\n",
    "\n",
    "print(f\"Total windows in train dataset: {total_train_windows}\")\n",
    "print(f\"Total windows in test dataset: {total_test_windows}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 포지셔널 인코딩 클래스\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 시계열 트랜스포머 모델 클래스\n",
    "class TimeSeriesTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model, n_heads, num_encoder_layers, d_ff, dropout_rate, lstm_hidden_size, num_lstm_layers):\n",
    "        super(TimeSeriesTransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=lstm_hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer to transform LSTM output to match Transformer d_model size\n",
    "        self.linear1 = nn.Linear(lstm_hidden_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_rate)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(src)\n",
    "\n",
    "        # Transform LSTM output to match Transformer d_model size\n",
    "        src = self.linear1(lstm_out) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Positional Encoding and Transformer\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        \n",
    "        # Using the last time step's output\n",
    "        output = torch.sigmoid(self.out(output[-1]))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No data found for index 460982",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\test2.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, criterion, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     test_loss \u001b[39m=\u001b[39m test(model, device, test_loader, criterion)\n",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\test2.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Github\\DL-FinalProject\\test2.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# Check if tensors list is empty\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tensors:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo data found for index \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m combined_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(tensors, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m# Here, ensure that combined_tensor has the expected shape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Github/DL-FinalProject/test2.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# and adjust as necessary\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: No data found for index 460982"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model\n",
    "num_features = 2  # Based on your dataset, adjust accordingly\n",
    "d_model = 512  # Transformer's feature size\n",
    "n_heads = 8  # Number of heads in multi-head attention\n",
    "num_encoder_layers = 6  # Number of encoder layers in the transformer\n",
    "d_ff = 2048  # Dimension of the feedforward network\n",
    "dropout_rate = 0.1\n",
    "lstm_hidden_size = 1000\n",
    "num_lstm_layers = 4\n",
    "\n",
    "model = TimeSeriesTransformerModel(num_features, d_model, n_heads, num_encoder_layers, d_ff, dropout_rate, lstm_hidden_size, num_lstm_layers)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "# Testing Loop\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, data).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "\n",
    "# Training and Testing\n",
    "num_epochs = 2\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test_loss = test(model, device, test_loader, criterion)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
